#7.非线性特征提取和模型堆叠

当在数据一个线性子空间像扁平饼时PCA是非常有用的。但是如果数据形成更复杂的形状呢？一个平面（线性子空间）可以推广到一个 *流形* （非线性子空间），它可以被认为是一个被各种拉伸和滚动的表面。

如果线性子空间是平的纸张，那么卷起的纸张就是非线性流形的例子。你也可以叫它瑞士卷。（见图7-1），一旦滚动，二维平面就会变为三维的。然而，它本质上仍是一个二维物体。换句话说，它具有低的内在维度，这是我们在“直觉”中已经接触到的一个概念。如果我们能以某种方式展开瑞士卷，我们就可以恢复到二维平面。这是非线性降维的目标，它假定流形比它所占据的全维更简单，并试图展开它。

![图7-1](../images/chapter_7/7-1.png)

关键是，即使当大流形看起来复杂，每个点周围的局部邻域通常可以很好地近似于一片平坦的表面。换句话说，他们学习使用局部结构对全局结构进行编码。非线性降维也被称为非线性嵌入，或流形学习。非线性嵌入可有效地将高维数据压缩成低维数据。它们通常用于2-D或3-D的可视化。

然而，特征工程的目的并不是要使特征维数尽可能低，而是要达到任务的正确特征。在这一章中，正确的特征是代表数据空间特征的特征。

聚类算法通常不是局部结构化学习的技术。但事实上也可以用他们这么做。彼此接近的点（由数据科学家使用某些度量可以定义的“接近度”）属于同一个簇。给定聚类，数据点可以由其聚类成员向量来表示。如果簇的数量小于原始的特征数，则新的表示将比原始的具有更小的维度；原始数据被压缩成较低的维度。

与非线性嵌入技术相比，聚类可以产生更多的特征。但是如果最终目标是特征工程而不是可视化，那这不是问题。

我们将提出一个使用k-均值聚类算法来进行结构化学习的思想。它简单易懂，易于实践。与非线性流体降维相反，k-均值执行非线性流形特征提取更容易解释。如果正确使用它，它可以是特征工程的一个强大的工具。

##k-均值聚类

k-均值是一种聚类算法。聚类算法根据数据在空间中的排列方式来分组数据。它们是无监督的，因为它们不需要任何类型的标签，使用算法仅基于数据本身的几何形状来推断聚类标签。

聚类算法依赖于 *度量* ，它是度量数据点之间的紧密度的测量。最流行的度量是欧几里德距离或欧几里得度量。它来自欧几里得几何学并测量两点之间的直线距离。我们对它很熟悉，因为这是我们在日常现实中看到的距离。

两个向量X和Y之间的欧几里得距离是X-Y的L2范数。（见L2范数的“L2标准化”），在数学语言中，它通常被写成 ‖ x -y ‖。

k-均值建立一个硬聚类，意味着每个数据点被分配给一个且只分配一个集群。该算法学习定位聚类中心，使得每个数据点和它的聚类中心之间的欧几里德距离的总和最小化。对于那些喜欢阅读公式而非语言的人来说，目标函数是：

![图E7-1](../images/chapter_7/E7-1.png)

每个群集C i包含数据点的子集。聚类i的中心等于簇中所有数据点的平均值： μ i = Σ x ∈ Ci x / n i , ，其中n i表示簇i中的数据点的数目。

图7-2显示了k-均值在两个不同的随机生成数据集上的工作。（a）中的数据是由具有相同方差但不同均值的随机高斯分布生成的。（c）中的数据是随机产生的。这些问题很容易解决，k-均值做得很好。（结果可能对簇的数目敏感，数目必须给算法）。这个例子的代码如例7-1所示。

![图7-2](../images/chapter_7/7-2.png)

例7-1
```python
import numpy as np 
from sklearn.cluster 
import KMeans from sklearn.datasets 
import make_blobs 
import matplotlib.pyplot as plt %matplotlib notebook 
 
n_data = 1000 
seed = 1 
n_clusters = 4 
 
# Generate random Gaussian blobs and run K-means 
blobs, blob_labels = make_blobs(n_samples=n_data, n_features=2,centers=n_centers, random_state=seed) 

clusters_blob = KMeans(n_clusters=n_centers, random_state=seed).fit_predict(blobs) 
 
# Generate data uniformly at random and run K-means 
uniform = np.random.rand(n_data, 2) 
clusters_uniform = KMeans(n_clusters=n_clusters, random_state=seed).fit_predict(uniform) 
 
# Matplotlib incantations for visualizing results 
figure = plt.figure() 
plt.subplot(221) 
plt.scatter(blobs[:, 0], blobs[:, 1], c=blob_labels, cmap='gist_rainbow') 
plt.title("(a) Four randomly generated blobs", fontsize=14) 
plt.axis('off') 
 
plt.subplot(222) 
plt.scatter(blobs[:, 0], blobs[:, 1], c=clusters_blob, cmap='gist_rainbow') 
plt.title("(b) Clusters found via K-means", fontsize=14) 
plt.axis('off') 
 
plt.subplot(223) 
plt.scatter(uniform[:, 0], uniform[:, 1]) 
plt.title("(c) 1000 randomly generated points", fontsize=14) 
plt.axis('off') 
 
plt.subplot(224) 
plt.scatter(uniform[:, 0], uniform[:, 1], c=clusters_uniform, cmap='gist_rainbow') 
plt.title("(d) Clusters found via K-means", fontsize=14) plt.axis('off')
```


![图7-2](../images/chapter_7/7-2.png)

##曲面拼接聚类

